{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleynakara/miniconda3/envs/ml4rg/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-25 14:27:03,069\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-06-25 14:27:03,129\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from timm.models.vision_transformer import vit_base_patch16_224\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "\n",
    "from models.simple_cnn import SimpleCNN\n",
    "from models.cnn_v2 import CNNV2\n",
    "from helpers.helpers import SequenceDataset\n",
    "from helpers.plots import plot_losses, plot_predictions_vs_labels, plot_predictions_vs_labels_by_species, plot_boxplot_predictions_vs_labels\n",
    "from helpers.early_stopping import EarlyStopping\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import gc\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# def load_dataframe():\n",
    "#     df = pd.read_csv(f'{os.getcwd()}/data/combined_data.csv')\n",
    "\n",
    "#     # Extract columns containing \"tpm\"\n",
    "#     tpm_columns = [col for col in df.columns if 'tpm' in col]\n",
    "\n",
    "#     # Create a new DataFrame with only tpm columns\n",
    "#     df_tpm = df[tpm_columns]\n",
    "\n",
    "#     # Extract prefixes\n",
    "#     prefixes = set(col.rsplit('_', 2)[0] for col in tpm_columns)\n",
    "\n",
    "#     # Calculate the mean of the columns with the same prefix\n",
    "#     mean_columns = {}\n",
    "#     for prefix in prefixes:\n",
    "#         mean_columns[prefix + '_mean'] = df_tpm.filter(like=prefix).mean(axis=1)\n",
    "\n",
    "#     # Create a DataFrame for the mean values\n",
    "#     df_means = pd.DataFrame(mean_columns)\n",
    "#     df.drop(columns=tpm_columns, inplace=True)\n",
    "#     df = pd.concat([df, df_means], axis=1)\n",
    "\n",
    "#     del df_means\n",
    "#     gc.collect()\n",
    "\n",
    "#     # Identify _mean columns\n",
    "#     mean_columns = [col for col in df.columns if '_mean' in col]\n",
    "\n",
    "#     # Drop rows where any of the _mean columns are equal to 0\n",
    "#     df = df[~(df[mean_columns] == 0).any(axis=1)]\n",
    "\n",
    "#     # Drop rows with missing upstream200 sequences\n",
    "#     df = df.dropna(subset=['upstream200'])\n",
    "\n",
    "#     # Drop rows with upstream200 sequences that contain anything but A, T, C, G\n",
    "#     df = df[df['upstream200'].apply(lambda x: set(x).issubset({'A', 'T', 'C', 'G'}))]\n",
    "\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     # Map each species_id to a one-hot encoding\n",
    "#     df['species_id'] = df['species_id'].apply(lambda x: [x])\n",
    "#     df['species_id'] = mlb.fit_transform(df['species_id']).tolist()\n",
    "\n",
    "#     # Map each base to one-hot encoding\n",
    "#     base_encodings = {'A': [1, 0, 0, 0], 'T': [0, 1, 0, 0], 'C': [0, 0, 1, 0], 'G': [0, 0, 0, 1]}\n",
    "#     longest_sequence = max(df['upstream200'].apply(lambda x: len(x)))\n",
    "#     df['upstream200'] = df['upstream200'].apply(lambda x: [base_encodings[base] for base in x] + [[0, 0, 0, 0]] * (longest_sequence - len(x)))\n",
    "\n",
    "#     df[mean_columns] = df[mean_columns].apply(np.log1p)\n",
    "#     print(\"The number of stress conditions is: \", len(mean_columns))\n",
    "#     # Create a new column 'stress' with a list of dictionaries for each stress condition\n",
    "#     df['stress'] = df.apply(lambda row: [{prefix: row[f\"{prefix}_mean\"]} for prefix in prefixes], axis=1)\n",
    "\n",
    "#     # Drop original mean columns\n",
    "#     df = df.drop(columns=mean_columns)\n",
    "\n",
    "#     # Explode the 'stress' column\n",
    "#     df = df.explode('stress').reset_index(drop=True)\n",
    "\n",
    "#     # Extract stress names and values\n",
    "#     df['stress_name'] = df['stress'].apply(lambda x: list(x.keys())[0])\n",
    "#     df['stress'] = df['stress'].apply(lambda x: list(x.values())[0])\n",
    "\n",
    "#     # One-hot encode stress names\n",
    "#     df['stress_name'] = df['stress_name'].apply(lambda x: [x])\n",
    "#     stress_one_hot_encoded = mlb.fit_transform(df['stress_name'])\n",
    "#     stress_one_hot_encoded_df = pd.DataFrame(stress_one_hot_encoded, columns=mlb.classes_)\n",
    "\n",
    "#     # Combine the one-hot encoded columns with the original DataFrame\n",
    "#     df = pd.concat([df, stress_one_hot_encoded_df], axis=1)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset(df, val_split=0.2, test_split=0.1):\n",
    "#     # split the data into training, validation, and testing\n",
    "#     X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "#         df[['species_id', 'stress_name', 'upstream200']], df['stress'], test_size=(val_split + test_split))\n",
    "#     X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_split/(val_split + test_split))\n",
    "\n",
    " \n",
    "#     # Create datasets\n",
    "#     train_dataset = SequenceDataset(X_train, torch.tensor(y_train.values).float())\n",
    "#     val_dataset = SequenceDataset(X_val, torch.tensor(y_val.values).float())\n",
    "#     test_dataset = SequenceDataset(X_test, torch.tensor(y_test.values).float())\n",
    "\n",
    "#     return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleynakara/miniconda3/envs/ml4rg/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, seq_len=203, num_features=4, dim=64, depth=6, heads=8, mlp_dim=128, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_features = num_features\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Linear transformation to embed input sequence\n",
    "        self.embedding = nn.Linear(num_features, dim)\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, dim))\n",
    "        \n",
    "        # Transformer Encoder Layers\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim, \n",
    "            nhead=heads, \n",
    "            num_encoder_layers=depth, \n",
    "            dim_feedforward=mlp_dim, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim)  # Output a feature vector\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, num_features)\n",
    "        \n",
    "        # Embed the input sequence\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x += self.pos_embedding\n",
    "        \n",
    "        # Transformer expects input of shape (seq_len, batch_size, dim)\n",
    "        x = x.squeeze(1).permute(1, 0, 2)  # (seq_len, batch_size, dim)\n",
    "        \n",
    "        # Pass through the transformer\n",
    "        x = self.transformer(x)  # (seq_len, batch_size, dim)\n",
    "        \n",
    "        # Take the mean of the output sequence (pooling)\n",
    "        x = x.mean(dim=0)  # (batch_size, dim)\n",
    "        \n",
    "        # Pass through the regression head\n",
    "        x = self.regression_head(x)  # (batch_size, dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SimpleViTModel(nn.Module):\n",
    "    def __init__(self, *kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        species_size = kwargs['species_size'] if 'species_size' in kwargs else 30\n",
    "        stress_condition_size = kwargs['stress_condition_size'] if 'stress_condition_size' in kwargs else 12\n",
    "        hidden_size = kwargs['hidden_size'] if 'hidden_size' in kwargs else 64\n",
    "        cnn_filers = kwargs['cnn_filers'] if 'cnn_filers' in kwargs else hidden_size\n",
    "\n",
    "        # Activation functions\n",
    "        self.activation = kwargs['activation'] if 'activation' in kwargs else nn.ReLU()\n",
    "\n",
    "        # Input layers for species and stress condition\n",
    "        self.input_species = nn.Linear(species_size, hidden_size)\n",
    "        self.input_stress_condition = nn.Linear(stress_condition_size, hidden_size)\n",
    "\n",
    "        # Vision Transformer for DNA sequence\n",
    "        self.vit = VisionTransformer(seq_len=203, num_features=4, dim=hidden_size, depth=6, heads=8, mlp_dim=128, dropout=0.1)\n",
    "\n",
    "        # Hidden layer\n",
    "        self.hidden = nn.Linear(hidden_size * 3, hidden_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_species, x_stress_condition, x_base = input\n",
    "\n",
    "        # Process species and stress condition\n",
    "        x_species = self.input_species(x_species)\n",
    "        x_stress_condition = self.input_stress_condition(x_stress_condition)\n",
    "\n",
    "        # Process DNA sequence with Vision Transformer\n",
    "        x_base = self.vit(x_base)\n",
    "\n",
    "        # Concatenate all features\n",
    "        x = torch.cat((x_species, x_stress_condition, x_base), dim=1)\n",
    "\n",
    "        # Hidden layer\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "species_size = 32\n",
    "stress_condition_size = 12\n",
    "hidden_size = 64\n",
    "batch_size = 1024\n",
    "\n",
    "config = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 100,\n",
    "    'species_id': -1,\n",
    "    'test_size': 20000,\n",
    "    'hidden_size': 64,\n",
    "    'cnn_filters': 100,\n",
    "    'model_version': 1,\n",
    "}\n",
    "\n",
    "\n",
    "model = SimpleViTModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_dataframe(data_df=None):\n",
    "    if data_df is not None:\n",
    "        return data_df\n",
    "    data_df = pd.read_csv(f'{os.getcwd()}/data/combined_data.csv')\n",
    "    # get mean of each stress condition\n",
    "    averages_df = data_df.copy()\n",
    "    stress_conditions = set([name.split('_')[0] for name in data_df.columns if 'TPM' in name])\n",
    "    for stress in stress_conditions:\n",
    "        stress_columns = [name for name in data_df.columns if stress+'_' in name]\n",
    "        averages_df[f'{stress}'] = np.mean([data_df[stress_columns[0]], data_df[stress_columns[1]], data_df[stress_columns[2]]], axis=0)\n",
    "\n",
    "    # Drop the columns that are not needed\n",
    "    averages_df = averages_df.drop(columns=[name for name in averages_df.columns if 'TPM' in name] + ['Chromosome','Region','Species', 'Unnamed: 0'])\n",
    "    # drop rows with missing upstream200 sequences\n",
    "    averages_df = averages_df.dropna(subset=['upstream200'])\n",
    "    # drop rows with upstream200 sequences that contain anything but A, T, C, G\n",
    "    averages_df = averages_df[averages_df['upstream200'].apply(lambda x: set(x).issubset({'A', 'T', 'C', 'G'}))]\n",
    "\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    # map each species id to a one hot encoding\n",
    "    averages_df['Species ID'] = averages_df['Species ID'].apply(lambda x: [x])\n",
    "    averages_df['Species ID'] = mlb.fit_transform(averages_df['Species ID']).tolist()\n",
    "\n",
    "    # map each base to one hot encoding\n",
    "    base_encodings = {'A': [1,0,0,0], 'T': [0,1,0,0], 'C': [0,0,1,0], 'G': [0,0,0,1]}\n",
    "    longest_sequence = max(averages_df['upstream200'].apply(lambda x: len(x)))\n",
    "    averages_df['upstream200'] = averages_df['upstream200'].apply(lambda x: [base_encodings[base] for base in x] + [[0,0,0,0]]*(longest_sequence-len(x)))\n",
    "\n",
    "    # explode dataset to have one row per stress condition\n",
    "    averages_df['Stress'] = averages_df.apply(lambda row: [{stress:row[stress]} for stress in stress_conditions], axis=1)\n",
    "    averages_df = averages_df.drop(columns=[name for name in averages_df.columns if name in stress_conditions])\n",
    "\n",
    "    averages_df = averages_df.explode('Stress')\n",
    "    averages_df['Stress_name'] = averages_df['Stress'].apply(lambda x: list(x.keys())[0])\n",
    "    averages_df['Stress'] = averages_df['Stress'].apply(lambda x: list(x.values())[0])\n",
    "\n",
    "    # one hot encode stress names\n",
    "    averages_df['Stress_name'] = averages_df['Stress_name'].apply(lambda x: [x])\n",
    "    averages_df['Stress_name'] = mlb.fit_transform(averages_df['Stress_name']).tolist()\n",
    "\n",
    "    # drop rows with 0 stress\n",
    "    averages_df = averages_df[averages_df['Stress'] > 0]\n",
    "\n",
    "    # log values of stress conditions\n",
    "    averages_df['Stress'] = averages_df['Stress'].apply(lambda x: np.log(x+1))\n",
    "\n",
    "    data_df = averages_df\n",
    "    return averages_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(species_id = -1, size = -1 ,val_split = 0.2 , test_split = 0.1, data_df=None):\n",
    "    data_df = load_dataframe(data_df)\n",
    "    \n",
    "    if species_id != -1:\n",
    "        data_df = data_df[data_df['Species ID'].apply(lambda x: x[species_id] == 1)]\n",
    "    if size != -1:\n",
    "        size = int(size * 1.39)\n",
    "        data_df = data_df.sample(size)\n",
    "\n",
    "    # data_df[\"upstream200\"] = data_df[\"upstream200\"].apply(lambda x: torch.cat([torch.tensor(x).float(), torch.zeros(224-len(x), 4)]))\n",
    "    \n",
    "    # split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_df[['Species ID', 'Stress_name', 'upstream200']], data_df['Stress'], test_size=test_split)\n",
    "\n",
    "    \n",
    "    # create a dataset\n",
    "    train_dataset = SequenceDataset(X_train, y_train)\n",
    "    test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = load_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 25019 samples, testing on 2780 samples\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_data(data_df=data_df, species_id=config['species_id'], size=config['test_size'])\n",
    "print(f\"Training on {len(train_dataset)} samples, testing on {len(test_dataset)} samples\")\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4rg/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4rg/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 89\u001b[0m, in \u001b[0;36mSimpleViTModel.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     86\u001b[0m x_stress_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_stress_condition(x_stress_condition)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Process DNA sequence with Vision Transformer\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m x_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Concatenate all features\u001b[39;00m\n\u001b[1;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_species, x_stress_condition, x_base), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4rg/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4rg/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (seq_len, batch_size, dim)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Pass through the transformer\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq_len, batch_size, dim)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Take the mean of the output sequence (pooling)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (batch_size, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4rg/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4rg/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss = evaluate(model, val_loader, criterion)\n",
    "print(f\"Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4rg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
