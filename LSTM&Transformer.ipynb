{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcPJ7VidcECx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9902452-1935-4b97-af57-83f1d199c5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             species  \\\n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                         upstream200    stress  \\\n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,...  0.033641   \n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,...  0.013922   \n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,... -0.806374   \n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,... -0.026784   \n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,...  0.922333   \n",
            "\n",
            "                         stress_name  \n",
            "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
            "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "0  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load data from the pickle file\n",
        "file_path = '/content/drive/MyDrive/MLRG/processed_data.pkl'\n",
        "with open(file_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Show the first few rows of the data\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Restore species integer values\n",
        "data['species'] = data['species'].apply(lambda x: np.argmax(x))\n",
        "\n",
        "# Base encodings\n",
        "base_encodings = {\n",
        "    (1, 0, 0, 0): \"A\",\n",
        "    (0, 1, 0, 0): \"T\",\n",
        "    (0, 0, 1, 0): \"C\",\n",
        "    (0, 0, 0, 1): \"G\"\n",
        "}\n",
        "\n",
        "# Function to decode the sequence and omit [0, 0, 0, 0]\n",
        "def decode_sequence(encoded_seq):\n",
        "    return ''.join([base_encodings[tuple(base)] for base in encoded_seq if tuple(base) in base_encodings])\n",
        "\n",
        "# Restore upstream200 sequences\n",
        "data['upstream200'] = data['upstream200'].apply(decode_sequence)\n",
        "\n",
        "# Restore stress_name integer values\n",
        "data['stress_name'] = data['stress_name'].apply(lambda x: np.argmax(x))\n",
        "\n",
        "# Show the restored data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iIOT8sEGy7J",
        "outputId": "f37aff75-365f-4e2e-ead0-5e7543acbdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             species  \\\n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                         upstream200    stress  \\\n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,...  0.033641   \n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,...  0.013922   \n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,... -0.806374   \n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,... -0.026784   \n",
            "0  [[0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0,...  0.922333   \n",
            "\n",
            "                         stress_name  \n",
            "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
            "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
            "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "0  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
            "   species                                        upstream200    stress  \\\n",
            "0        0  CCTTCCAAGCTTACGACGAGGGTTCGATTCCCTTCACCCGCTCCAA...  0.033641   \n",
            "0        0  CCTTCCAAGCTTACGACGAGGGTTCGATTCCCTTCACCCGCTCCAA...  0.013922   \n",
            "0        0  CCTTCCAAGCTTACGACGAGGGTTCGATTCCCTTCACCCGCTCCAA... -0.806374   \n",
            "0        0  CCTTCCAAGCTTACGACGAGGGTTCGATTCCCTTCACCCGCTCCAA... -0.026784   \n",
            "0        0  CCTTCCAAGCTTACGACGAGGGTTCGATTCCCTTCACCCGCTCCAA...  0.922333   \n",
            "\n",
            "   stress_name  \n",
            "0           10  \n",
            "0            1  \n",
            "0            4  \n",
            "0            0  \n",
            "0            8  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "k9XeeZFcMwmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 将嵌套列表转换为NumPy数组（假设数据已经是列表格式）\n",
        "data['species'] = data['species'].apply(lambda x: np.array(x))\n",
        "data['upstream200'] = data['upstream200'].apply(lambda x: np.array(x))\n",
        "data['stress_name'] = data['stress_name'].apply(lambda x: np.array(x))\n",
        "\n",
        "# 准备输入和输出数据\n",
        "X_species = np.vstack(data['species'].values)\n",
        "X_upstream200 = np.stack(data['upstream200'].values)\n",
        "X_stress_name = np.vstack(data['stress_name'].values)\n",
        "y_stress = data['stress'].values\n",
        "\n",
        "# 对 upstream200 数据进行标准化\n",
        "upstream200_shape = X_upstream200.shape\n",
        "scaler = StandardScaler()\n",
        "X_upstream200 = scaler.fit_transform(X_upstream200.reshape(-1, upstream200_shape[-1])).reshape(upstream200_shape)\n",
        "\n",
        "# 拆分训练集和测试集\n",
        "X_train_species, X_test_species, X_train_upstream200, X_test_upstream200, X_train_stress_name, X_test_stress_name, y_train, y_test = train_test_split(\n",
        "    X_species, X_upstream200, X_stress_name, y_stress, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 转换为张量\n",
        "X_train_species_tensor = torch.tensor(X_train_species, dtype=torch.long)\n",
        "X_test_species_tensor = torch.tensor(X_test_species, dtype=torch.long)\n",
        "X_train_upstream200_tensor = torch.tensor(X_train_upstream200, dtype=torch.float32)\n",
        "X_test_upstream200_tensor = torch.tensor(X_test_upstream200, dtype=torch.float32)\n",
        "X_train_stress_name_tensor = torch.tensor(X_train_stress_name, dtype=torch.long)\n",
        "X_test_stress_name_tensor = torch.tensor(X_test_stress_name, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# 创建数据加载器\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train_species_tensor, X_train_upstream200_tensor, X_train_stress_name_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_species_tensor, X_test_upstream200_tensor, X_test_stress_name_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "9j-O-AhpYo26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTMWithEmbedding(nn.Module):\n",
        "    def __init__(self, species_dim, stress_name_dim, embedding_dim, input_dim_upstream200, hidden_dim, output_dim, num_layers):\n",
        "        super(SimpleLSTMWithEmbedding, self).__init__()\n",
        "        self.species_embedding = nn.Embedding(species_dim, embedding_dim)\n",
        "        self.stress_name_embedding = nn.Embedding(stress_name_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim * 2 + input_dim_upstream200, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, species, upstream200, stress_name):\n",
        "        species_embedded = self.species_embedding(species).sum(dim=1)\n",
        "        stress_name_embedded = self.stress_name_embedding(stress_name).sum(dim=1)\n",
        "        x = torch.cat((species_embedded, upstream200.view(upstream200.size(0), -1), stress_name_embedded), dim=1).unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        x = self.fc(lstm_out[:, -1, :])\n",
        "        return x\n",
        "\n",
        "# 确保 species_dim 和 stress_name_dim 大于它们的最大索引值\n",
        "species_dim = X_train_species_tensor.max() + 1\n",
        "stress_name_dim = X_train_stress_name_tensor.max() + 1\n",
        "embedding_dim = 32  # 嵌入维度\n",
        "input_dim_upstream200 = X_train_upstream200_tensor.shape[1] * X_train_upstream200_tensor.shape[2]\n",
        "hidden_dim = 256  # 增加LSTM 隐藏层维度\n",
        "output_dim = 1\n",
        "num_layers = 3  # 增加 LSTM 层数\n",
        "\n",
        "# 初始化模型\n",
        "model = SimpleLSTMWithEmbedding(species_dim, stress_name_dim, embedding_dim, input_dim_upstream200, hidden_dim, output_dim, num_layers)\n"
      ],
      "metadata": {
        "id": "HgQ2M8yOMv_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义损失函数和优化器\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "best_model_path = '/content/drive/MyDrive/MLRG/best_model_weights.pth'\n",
        "\n",
        "# 训练模型\n",
        "num_epochs = 300\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    total_batches = len(train_loader)\n",
        "    for batch_idx, (batch_species, batch_upstream200, batch_stress_name, batch_y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_species, batch_upstream200, batch_stress_name)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # 打印epoch的平均损失\n",
        "    epoch_loss /= total_batches\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # 在测试集上评估模型\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_species, batch_upstream200, batch_stress_name, batch_y in test_loader:\n",
        "            outputs = model(batch_species, batch_upstream200, batch_stress_name)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    # 保存当前模型权重\n",
        "    # torch.save(model.state_dict(), '/content/drive/MyDrive/MLRG/model_epoch_{}.pth'.format(epoch + 1))\n",
        "\n",
        "    # 如果测试损失比最佳损失更低，保存当前模型的权重为最佳模型\n",
        "    if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Best model weights saved to {best_model_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DFmiF_HZLIP",
        "outputId": "545c8baf-5f5e-45bf-fc39-713314afa125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Average Loss: 0.7780\n",
            "Epoch [1/300], Test Loss: 0.7771\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [2/300], Average Loss: 0.7779\n",
            "Epoch [2/300], Test Loss: 0.7771\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [3/300], Average Loss: 0.7779\n",
            "Epoch [3/300], Test Loss: 0.7771\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [4/300], Average Loss: 0.7779\n",
            "Epoch [4/300], Test Loss: 0.7771\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [5/300], Average Loss: 0.7779\n",
            "Epoch [5/300], Test Loss: 0.7774\n",
            "Epoch [6/300], Average Loss: 0.7779\n",
            "Epoch [6/300], Test Loss: 0.7772\n",
            "Epoch [7/300], Average Loss: 0.7779\n",
            "Epoch [7/300], Test Loss: 0.7771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformer"
      ],
      "metadata": {
        "id": "lMwlxRq2QDDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['species'] = data['species'].apply(lambda x: np.argmax(x))\n",
        "data['stress_name'] = data['stress_name'].apply(lambda x: np.argmax(x))"
      ],
      "metadata": {
        "id": "rg4dnqYbQdI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "data['species'] = data['species'].apply(lambda x: np.array(x))\n",
        "data['upstream200'] = data['upstream200'].apply(lambda x: np.array(x))\n",
        "data['stress_name'] = data['stress_name'].apply(lambda x: np.array(x))\n",
        "\n",
        "# 准备输入和输出数据\n",
        "X_species = np.vstack(data['species'].values)\n",
        "X_upstream200 = np.stack(data['upstream200'].values)\n",
        "X_stress_name = np.vstack(data['stress_name'].values)\n",
        "y_stress = data['stress'].values\n",
        "\n",
        "# 拆分训练集和测试集\n",
        "X_train_species, X_test_species, X_train_upstream200, X_test_upstream200, X_train_stress_name, X_test_stress_name, y_train, y_test = train_test_split(\n",
        "    X_species, X_upstream200, X_stress_name, y_stress, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 转换为张量\n",
        "X_train_species_tensor = torch.tensor(X_train_species, dtype=torch.long)\n",
        "X_test_species_tensor = torch.tensor(X_test_species, dtype=torch.long)\n",
        "X_train_upstream200_tensor = torch.tensor(X_train_upstream200, dtype=torch.float32)\n",
        "X_test_upstream200_tensor = torch.tensor(X_test_upstream200, dtype=torch.float32)\n",
        "X_train_stress_name_tensor = torch.tensor(X_train_stress_name, dtype=torch.long)\n",
        "X_test_stress_name_tensor = torch.tensor(X_test_stress_name, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
      ],
      "metadata": {
        "id": "FhXyZlgAOIlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pickle\n",
        "\n",
        "# Load data from the pickle file\n",
        "file_path = '/content/drive/MyDrive/MLRG/processed_data.pkl'\n",
        "with open(file_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# 将嵌套列表转换为NumPy数组（假设数据已经是列表格式）\n",
        "data['species'] = data['species'].apply(lambda x: np.array(x))\n",
        "data['upstream200'] = data['upstream200'].apply(lambda x: np.array(x))\n",
        "data['stress_name'] = data['stress_name'].apply(lambda x: np.array(x))\n",
        "\n",
        "# 准备输入和输出数据\n",
        "X_species = np.vstack(data['species'].values)\n",
        "X_upstream200 = np.stack(data['upstream200'].values)\n",
        "X_stress_name = np.vstack(data['stress_name'].values)\n",
        "y_stress = data['stress'].values\n",
        "\n",
        "# 拆分训练集和测试集\n",
        "X_train_species, X_test_species, X_train_upstream200, X_test_upstream200, X_train_stress_name, X_test_stress_name, y_train, y_test = train_test_split(\n",
        "    X_species, X_upstream200, X_stress_name, y_stress, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 转换为张量\n",
        "X_train_species_tensor = torch.tensor(X_train_species, dtype=torch.long)\n",
        "X_test_species_tensor = torch.tensor(X_test_species, dtype=torch.long)\n",
        "X_train_upstream200_tensor = torch.tensor(X_train_upstream200, dtype=torch.float32)\n",
        "X_test_upstream200_tensor = torch.tensor(X_test_upstream200, dtype=torch.float32)\n",
        "X_train_stress_name_tensor = torch.tensor(X_train_stress_name, dtype=torch.long)\n",
        "X_test_stress_name_tensor = torch.tensor(X_test_stress_name, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c7n58KZIU_Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建数据加载器\n",
        "batch_size = 128\n",
        "train_dataset = TensorDataset(X_train_species_tensor, X_train_upstream200_tensor, X_train_stress_name_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_species_tensor, X_test_upstream200_tensor, X_test_stress_name_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "fpSkbAinYi5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformerWithEmbedding(nn.Module):\n",
        "    def __init__(self, species_dim, stress_name_dim, embedding_dim, input_dim_upstream200, output_dim, nhead, num_layers, dim_feedforward):\n",
        "        super(SimpleTransformerWithEmbedding, self).__init__()\n",
        "        self.species_embedding = nn.Embedding(species_dim, embedding_dim)\n",
        "        self.stress_name_embedding = nn.Embedding(stress_name_dim, embedding_dim)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim * 2 + input_dim_upstream200, nhead=nhead, dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(embedding_dim * 2 + input_dim_upstream200, output_dim)\n",
        "\n",
        "    def forward(self, species, upstream200, stress_name):\n",
        "        species_embedded = self.species_embedding(species).sum(dim=1)\n",
        "        stress_name_embedded = self.stress_name_embedding(stress_name).sum(dim=1)\n",
        "        x = torch.cat((species_embedded, upstream200.view(upstream200.size(0), -1), stress_name_embedded), dim=1)\n",
        "        x = self.transformer_encoder(x.unsqueeze(0)).squeeze(0)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# 确保 species_dim 和 stress_name_dim 大于它们的最大索引值\n",
        "species_dim = X_train_species_tensor.max() + 1\n",
        "stress_name_dim = X_train_stress_name_tensor.max() + 1\n",
        "embedding_dim = 32  # 降低嵌入维度\n",
        "input_dim_upstream200 = X_train_upstream200_tensor.shape[1] * X_train_upstream200_tensor.shape[2]\n",
        "output_dim = 1\n",
        "nhead = 1\n",
        "num_layers = 1  # 减少 Transformer 层数\n",
        "dim_feedforward = 64  # 降低前馈神经网络维度\n",
        "\n",
        "# 初始化模型\n",
        "model = SimpleTransformerWithEmbedding(species_dim, stress_name_dim, embedding_dim, input_dim_upstream200, output_dim, nhead, num_layers, dim_feedforward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2h2jaWyRVp6",
        "outputId": "0f3e59b7-d2b7-437f-dd75-e79367fb7834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Max species index: {X_train_species.max()}\")\n",
        "print(f\"Max stress_name index: {X_train_stress_name.max()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpmHeCAEUrDW",
        "outputId": "68d3cc09-e59c-40e4-ce14-4d4eda2e1c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max species index: 29\n",
            "Max stress_name index: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义损失函数和优化器\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "best_model_path = '/content/drive/MyDrive/MLRG/best_model_weights.pth'\n",
        "\n",
        "# 训练模型\n",
        "num_epochs = 300\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    total_batches = len(train_loader)\n",
        "    for batch_idx, (batch_species, batch_upstream200, batch_stress_name, batch_y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_species, batch_upstream200, batch_stress_name)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # 打印epoch的平均损失\n",
        "    epoch_loss /= total_batches\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # 在测试集上评估模型\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_species, batch_upstream200, batch_stress_name, batch_y in test_loader:\n",
        "            outputs = model(batch_species, batch_upstream200, batch_stress_name)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    # 保存当前模型权重\n",
        "    # torch.save(model.state_dict(), '/content/drive/MyDrive/MLRG/model_epoch_{}.pth'.format(epoch + 1))\n",
        "\n",
        "    # 如果测试损失比最佳损失更低，保存当前模型的权重为最佳模型\n",
        "    if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Best model weights saved to {best_model_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dNqnhEnARC3c",
        "outputId": "f7e35993-4a7b-4466-9806-50b81a9becd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Average Loss: 0.7166\n",
            "Epoch [1/300], Test Loss: 0.7329\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [2/300], Average Loss: 0.7165\n",
            "Epoch [2/300], Test Loss: 0.7322\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [3/300], Average Loss: 0.7148\n",
            "Epoch [3/300], Test Loss: 0.7335\n",
            "Epoch [4/300], Average Loss: 0.7138\n",
            "Epoch [4/300], Test Loss: 0.7263\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [5/300], Average Loss: 0.7134\n",
            "Epoch [5/300], Test Loss: 0.7262\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [6/300], Average Loss: 0.7140\n",
            "Epoch [6/300], Test Loss: 0.7332\n",
            "Epoch [7/300], Average Loss: 0.7123\n",
            "Epoch [7/300], Test Loss: 0.7236\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [8/300], Average Loss: 0.7120\n",
            "Epoch [8/300], Test Loss: 0.7327\n",
            "Epoch [9/300], Average Loss: 0.7107\n",
            "Epoch [9/300], Test Loss: 0.7315\n",
            "Epoch [10/300], Average Loss: 0.7106\n",
            "Epoch [10/300], Test Loss: 0.7256\n",
            "Epoch [11/300], Average Loss: 0.7105\n",
            "Epoch [11/300], Test Loss: 0.7342\n",
            "Epoch [12/300], Average Loss: 0.7087\n",
            "Epoch [12/300], Test Loss: 0.7231\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [13/300], Average Loss: 0.7088\n",
            "Epoch [13/300], Test Loss: 0.7203\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [14/300], Average Loss: 0.7086\n",
            "Epoch [14/300], Test Loss: 0.7233\n",
            "Epoch [15/300], Average Loss: 0.7082\n",
            "Epoch [15/300], Test Loss: 0.7215\n",
            "Epoch [16/300], Average Loss: 0.7078\n",
            "Epoch [16/300], Test Loss: 0.7207\n",
            "Epoch [17/300], Average Loss: 0.7073\n",
            "Epoch [17/300], Test Loss: 0.7223\n",
            "Epoch [18/300], Average Loss: 0.7067\n",
            "Epoch [18/300], Test Loss: 0.7310\n",
            "Epoch [19/300], Average Loss: 0.7058\n",
            "Epoch [19/300], Test Loss: 0.7214\n",
            "Epoch [20/300], Average Loss: 0.7053\n",
            "Epoch [20/300], Test Loss: 0.7202\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [21/300], Average Loss: 0.7048\n",
            "Epoch [21/300], Test Loss: 0.7251\n",
            "Epoch [22/300], Average Loss: 0.7052\n",
            "Epoch [22/300], Test Loss: 0.7206\n",
            "Epoch [23/300], Average Loss: 0.7033\n",
            "Epoch [23/300], Test Loss: 0.7299\n",
            "Epoch [24/300], Average Loss: 0.7035\n",
            "Epoch [24/300], Test Loss: 0.7196\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [25/300], Average Loss: 0.7026\n",
            "Epoch [25/300], Test Loss: 0.7203\n",
            "Epoch [26/300], Average Loss: 0.7027\n",
            "Epoch [26/300], Test Loss: 0.7217\n",
            "Epoch [27/300], Average Loss: 0.7029\n",
            "Epoch [27/300], Test Loss: 0.7202\n",
            "Epoch [28/300], Average Loss: 0.7008\n",
            "Epoch [28/300], Test Loss: 0.7240\n",
            "Epoch [29/300], Average Loss: 0.7012\n",
            "Epoch [29/300], Test Loss: 0.7218\n",
            "Epoch [30/300], Average Loss: 0.7006\n",
            "Epoch [30/300], Test Loss: 0.7236\n",
            "Epoch [31/300], Average Loss: 0.7003\n",
            "Epoch [31/300], Test Loss: 0.7361\n",
            "Epoch [32/300], Average Loss: 0.6988\n",
            "Epoch [32/300], Test Loss: 0.7170\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [33/300], Average Loss: 0.6992\n",
            "Epoch [33/300], Test Loss: 0.7182\n",
            "Epoch [34/300], Average Loss: 0.6989\n",
            "Epoch [34/300], Test Loss: 0.7133\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [35/300], Average Loss: 0.6972\n",
            "Epoch [35/300], Test Loss: 0.7243\n",
            "Epoch [36/300], Average Loss: 0.6984\n",
            "Epoch [36/300], Test Loss: 0.7214\n",
            "Epoch [37/300], Average Loss: 0.6977\n",
            "Epoch [37/300], Test Loss: 0.7124\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [38/300], Average Loss: 0.6974\n",
            "Epoch [38/300], Test Loss: 0.7181\n",
            "Epoch [39/300], Average Loss: 0.6966\n",
            "Epoch [39/300], Test Loss: 0.7281\n",
            "Epoch [40/300], Average Loss: 0.6971\n",
            "Epoch [40/300], Test Loss: 0.7156\n",
            "Epoch [41/300], Average Loss: 0.6951\n",
            "Epoch [41/300], Test Loss: 0.7186\n",
            "Epoch [42/300], Average Loss: 0.6955\n",
            "Epoch [42/300], Test Loss: 0.7207\n",
            "Epoch [43/300], Average Loss: 0.6962\n",
            "Epoch [43/300], Test Loss: 0.7169\n",
            "Epoch [44/300], Average Loss: 0.6950\n",
            "Epoch [44/300], Test Loss: 0.7176\n",
            "Epoch [45/300], Average Loss: 0.6941\n",
            "Epoch [45/300], Test Loss: 0.7168\n",
            "Epoch [46/300], Average Loss: 0.6943\n",
            "Epoch [46/300], Test Loss: 0.7122\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [47/300], Average Loss: 0.6942\n",
            "Epoch [47/300], Test Loss: 0.7212\n",
            "Epoch [48/300], Average Loss: 0.6925\n",
            "Epoch [48/300], Test Loss: 0.7166\n",
            "Epoch [49/300], Average Loss: 0.6924\n",
            "Epoch [49/300], Test Loss: 0.7152\n",
            "Epoch [50/300], Average Loss: 0.6922\n",
            "Epoch [50/300], Test Loss: 0.7083\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [51/300], Average Loss: 0.6917\n",
            "Epoch [51/300], Test Loss: 0.7157\n",
            "Epoch [52/300], Average Loss: 0.6922\n",
            "Epoch [52/300], Test Loss: 0.7141\n",
            "Epoch [53/300], Average Loss: 0.6910\n",
            "Epoch [53/300], Test Loss: 0.7137\n",
            "Epoch [54/300], Average Loss: 0.6902\n",
            "Epoch [54/300], Test Loss: 0.7171\n",
            "Epoch [55/300], Average Loss: 0.6897\n",
            "Epoch [55/300], Test Loss: 0.7137\n",
            "Epoch [56/300], Average Loss: 0.6913\n",
            "Epoch [56/300], Test Loss: 0.7226\n",
            "Epoch [57/300], Average Loss: 0.6897\n",
            "Epoch [57/300], Test Loss: 0.7188\n",
            "Epoch [58/300], Average Loss: 0.6891\n",
            "Epoch [58/300], Test Loss: 0.7159\n",
            "Epoch [59/300], Average Loss: 0.6889\n",
            "Epoch [59/300], Test Loss: 0.7204\n",
            "Epoch [60/300], Average Loss: 0.6886\n",
            "Epoch [60/300], Test Loss: 0.7166\n",
            "Epoch [61/300], Average Loss: 0.6891\n",
            "Epoch [61/300], Test Loss: 0.7150\n",
            "Epoch [62/300], Average Loss: 0.6878\n",
            "Epoch [62/300], Test Loss: 0.7076\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [63/300], Average Loss: 0.6866\n",
            "Epoch [63/300], Test Loss: 0.7215\n",
            "Epoch [64/300], Average Loss: 0.6878\n",
            "Epoch [64/300], Test Loss: 0.7209\n",
            "Epoch [65/300], Average Loss: 0.6877\n",
            "Epoch [65/300], Test Loss: 0.7157\n",
            "Epoch [66/300], Average Loss: 0.6870\n",
            "Epoch [66/300], Test Loss: 0.7105\n",
            "Epoch [67/300], Average Loss: 0.6851\n",
            "Epoch [67/300], Test Loss: 0.7155\n",
            "Epoch [68/300], Average Loss: 0.6861\n",
            "Epoch [68/300], Test Loss: 0.7065\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [69/300], Average Loss: 0.6858\n",
            "Epoch [69/300], Test Loss: 0.7154\n",
            "Epoch [70/300], Average Loss: 0.6853\n",
            "Epoch [70/300], Test Loss: 0.7278\n",
            "Epoch [71/300], Average Loss: 0.6854\n",
            "Epoch [71/300], Test Loss: 0.7195\n",
            "Epoch [72/300], Average Loss: 0.6854\n",
            "Epoch [72/300], Test Loss: 0.7124\n",
            "Epoch [73/300], Average Loss: 0.6842\n",
            "Epoch [73/300], Test Loss: 0.7110\n",
            "Epoch [74/300], Average Loss: 0.6832\n",
            "Epoch [74/300], Test Loss: 0.7109\n",
            "Epoch [75/300], Average Loss: 0.6844\n",
            "Epoch [75/300], Test Loss: 0.7092\n",
            "Epoch [76/300], Average Loss: 0.6833\n",
            "Epoch [76/300], Test Loss: 0.7120\n",
            "Epoch [77/300], Average Loss: 0.6826\n",
            "Epoch [77/300], Test Loss: 0.7110\n",
            "Epoch [78/300], Average Loss: 0.6830\n",
            "Epoch [78/300], Test Loss: 0.7207\n",
            "Epoch [79/300], Average Loss: 0.6817\n",
            "Epoch [79/300], Test Loss: 0.7125\n",
            "Epoch [80/300], Average Loss: 0.6817\n",
            "Epoch [80/300], Test Loss: 0.7165\n",
            "Epoch [81/300], Average Loss: 0.6814\n",
            "Epoch [81/300], Test Loss: 0.7321\n",
            "Epoch [82/300], Average Loss: 0.6818\n",
            "Epoch [82/300], Test Loss: 0.7054\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [83/300], Average Loss: 0.6812\n",
            "Epoch [83/300], Test Loss: 0.7179\n",
            "Epoch [84/300], Average Loss: 0.6804\n",
            "Epoch [84/300], Test Loss: 0.7043\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [85/300], Average Loss: 0.6802\n",
            "Epoch [85/300], Test Loss: 0.7079\n",
            "Epoch [86/300], Average Loss: 0.6797\n",
            "Epoch [86/300], Test Loss: 0.7125\n",
            "Epoch [87/300], Average Loss: 0.6795\n",
            "Epoch [87/300], Test Loss: 0.7051\n",
            "Epoch [88/300], Average Loss: 0.6798\n",
            "Epoch [88/300], Test Loss: 0.7039\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [89/300], Average Loss: 0.6793\n",
            "Epoch [89/300], Test Loss: 0.7040\n",
            "Epoch [90/300], Average Loss: 0.6788\n",
            "Epoch [90/300], Test Loss: 0.6992\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [91/300], Average Loss: 0.6803\n",
            "Epoch [91/300], Test Loss: 0.7208\n",
            "Epoch [92/300], Average Loss: 0.6783\n",
            "Epoch [92/300], Test Loss: 0.7124\n",
            "Epoch [93/300], Average Loss: 0.6789\n",
            "Epoch [93/300], Test Loss: 0.7080\n",
            "Epoch [94/300], Average Loss: 0.6778\n",
            "Epoch [94/300], Test Loss: 0.7095\n",
            "Epoch [95/300], Average Loss: 0.6779\n",
            "Epoch [95/300], Test Loss: 0.7060\n",
            "Epoch [96/300], Average Loss: 0.6778\n",
            "Epoch [96/300], Test Loss: 0.7050\n",
            "Epoch [97/300], Average Loss: 0.6766\n",
            "Epoch [97/300], Test Loss: 0.7119\n",
            "Epoch [98/300], Average Loss: 0.6774\n",
            "Epoch [98/300], Test Loss: 0.7030\n",
            "Epoch [99/300], Average Loss: 0.6769\n",
            "Epoch [99/300], Test Loss: 0.7069\n",
            "Epoch [100/300], Average Loss: 0.6768\n",
            "Epoch [100/300], Test Loss: 0.7025\n",
            "Epoch [101/300], Average Loss: 0.6761\n",
            "Epoch [101/300], Test Loss: 0.7020\n",
            "Epoch [102/300], Average Loss: 0.6771\n",
            "Epoch [102/300], Test Loss: 0.7094\n",
            "Epoch [103/300], Average Loss: 0.6745\n",
            "Epoch [103/300], Test Loss: 0.7201\n",
            "Epoch [104/300], Average Loss: 0.6750\n",
            "Epoch [104/300], Test Loss: 0.7126\n",
            "Epoch [105/300], Average Loss: 0.6755\n",
            "Epoch [105/300], Test Loss: 0.7009\n",
            "Epoch [106/300], Average Loss: 0.6748\n",
            "Epoch [106/300], Test Loss: 0.6992\n",
            "Epoch [107/300], Average Loss: 0.6749\n",
            "Epoch [107/300], Test Loss: 0.7021\n",
            "Epoch [108/300], Average Loss: 0.6747\n",
            "Epoch [108/300], Test Loss: 0.7013\n",
            "Epoch [109/300], Average Loss: 0.6737\n",
            "Epoch [109/300], Test Loss: 0.7064\n",
            "Epoch [110/300], Average Loss: 0.6728\n",
            "Epoch [110/300], Test Loss: 0.7008\n",
            "Epoch [111/300], Average Loss: 0.6728\n",
            "Epoch [111/300], Test Loss: 0.7071\n",
            "Epoch [112/300], Average Loss: 0.6745\n",
            "Epoch [112/300], Test Loss: 0.7040\n",
            "Epoch [113/300], Average Loss: 0.6737\n",
            "Epoch [113/300], Test Loss: 0.7082\n",
            "Epoch [114/300], Average Loss: 0.6729\n",
            "Epoch [114/300], Test Loss: 0.7087\n",
            "Epoch [115/300], Average Loss: 0.6732\n",
            "Epoch [115/300], Test Loss: 0.7017\n",
            "Epoch [116/300], Average Loss: 0.6723\n",
            "Epoch [116/300], Test Loss: 0.7052\n",
            "Epoch [117/300], Average Loss: 0.6720\n",
            "Epoch [117/300], Test Loss: 0.7110\n",
            "Epoch [118/300], Average Loss: 0.6731\n",
            "Epoch [118/300], Test Loss: 0.7040\n",
            "Epoch [119/300], Average Loss: 0.6728\n",
            "Epoch [119/300], Test Loss: 0.7002\n",
            "Epoch [120/300], Average Loss: 0.6713\n",
            "Epoch [120/300], Test Loss: 0.7079\n",
            "Epoch [121/300], Average Loss: 0.6714\n",
            "Epoch [121/300], Test Loss: 0.6980\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [122/300], Average Loss: 0.6715\n",
            "Epoch [122/300], Test Loss: 0.7049\n",
            "Epoch [123/300], Average Loss: 0.6705\n",
            "Epoch [123/300], Test Loss: 0.7178\n",
            "Epoch [124/300], Average Loss: 0.6715\n",
            "Epoch [124/300], Test Loss: 0.6958\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [125/300], Average Loss: 0.6698\n",
            "Epoch [125/300], Test Loss: 0.7002\n",
            "Epoch [126/300], Average Loss: 0.6694\n",
            "Epoch [126/300], Test Loss: 0.7089\n",
            "Epoch [127/300], Average Loss: 0.6697\n",
            "Epoch [127/300], Test Loss: 0.7232\n",
            "Epoch [128/300], Average Loss: 0.6688\n",
            "Epoch [128/300], Test Loss: 0.7040\n",
            "Epoch [129/300], Average Loss: 0.6704\n",
            "Epoch [129/300], Test Loss: 0.7066\n",
            "Epoch [130/300], Average Loss: 0.6697\n",
            "Epoch [130/300], Test Loss: 0.7054\n",
            "Epoch [131/300], Average Loss: 0.6697\n",
            "Epoch [131/300], Test Loss: 0.7047\n",
            "Epoch [132/300], Average Loss: 0.6697\n",
            "Epoch [132/300], Test Loss: 0.7043\n",
            "Epoch [133/300], Average Loss: 0.6703\n",
            "Epoch [133/300], Test Loss: 0.7013\n",
            "Epoch [134/300], Average Loss: 0.6696\n",
            "Epoch [134/300], Test Loss: 0.6994\n",
            "Epoch [135/300], Average Loss: 0.6668\n",
            "Epoch [135/300], Test Loss: 0.7008\n",
            "Epoch [136/300], Average Loss: 0.6685\n",
            "Epoch [136/300], Test Loss: 0.7039\n",
            "Epoch [137/300], Average Loss: 0.6695\n",
            "Epoch [137/300], Test Loss: 0.7012\n",
            "Epoch [138/300], Average Loss: 0.6695\n",
            "Epoch [138/300], Test Loss: 0.6974\n",
            "Epoch [139/300], Average Loss: 0.6687\n",
            "Epoch [139/300], Test Loss: 0.7030\n",
            "Epoch [140/300], Average Loss: 0.6678\n",
            "Epoch [140/300], Test Loss: 0.7011\n",
            "Epoch [141/300], Average Loss: 0.6676\n",
            "Epoch [141/300], Test Loss: 0.7130\n",
            "Epoch [142/300], Average Loss: 0.6679\n",
            "Epoch [142/300], Test Loss: 0.6950\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [143/300], Average Loss: 0.6676\n",
            "Epoch [143/300], Test Loss: 0.7026\n",
            "Epoch [144/300], Average Loss: 0.6663\n",
            "Epoch [144/300], Test Loss: 0.7007\n",
            "Epoch [145/300], Average Loss: 0.6665\n",
            "Epoch [145/300], Test Loss: 0.6947\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [146/300], Average Loss: 0.6661\n",
            "Epoch [146/300], Test Loss: 0.7142\n",
            "Epoch [147/300], Average Loss: 0.6665\n",
            "Epoch [147/300], Test Loss: 0.7005\n",
            "Epoch [148/300], Average Loss: 0.6657\n",
            "Epoch [148/300], Test Loss: 0.7052\n",
            "Epoch [149/300], Average Loss: 0.6671\n",
            "Epoch [149/300], Test Loss: 0.6967\n",
            "Epoch [150/300], Average Loss: 0.6660\n",
            "Epoch [150/300], Test Loss: 0.7022\n",
            "Epoch [151/300], Average Loss: 0.6663\n",
            "Epoch [151/300], Test Loss: 0.7012\n",
            "Epoch [152/300], Average Loss: 0.6719\n",
            "Epoch [152/300], Test Loss: 0.7046\n",
            "Epoch [153/300], Average Loss: 0.6656\n",
            "Epoch [153/300], Test Loss: 0.6916\n",
            "Best model weights saved to /content/drive/MyDrive/MLRG/best_model_weights.pth\n",
            "Epoch [154/300], Average Loss: 0.6657\n",
            "Epoch [154/300], Test Loss: 0.7002\n",
            "Epoch [155/300], Average Loss: 0.6659\n",
            "Epoch [155/300], Test Loss: 0.6935\n",
            "Epoch [156/300], Average Loss: 0.6656\n",
            "Epoch [156/300], Test Loss: 0.7080\n",
            "Epoch [157/300], Average Loss: 0.6643\n",
            "Epoch [157/300], Test Loss: 0.6945\n",
            "Epoch [158/300], Average Loss: 0.6647\n",
            "Epoch [158/300], Test Loss: 0.6952\n",
            "Epoch [159/300], Average Loss: 0.6641\n",
            "Epoch [159/300], Test Loss: 0.7057\n",
            "Epoch [160/300], Average Loss: 0.6642\n",
            "Epoch [160/300], Test Loss: 0.6972\n",
            "Epoch [161/300], Average Loss: 0.6637\n",
            "Epoch [161/300], Test Loss: 0.7034\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d1870ff1a744>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 state_steps)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    319\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train = []\n",
        "    y_true_train = []\n",
        "    for batch_species, batch_upstream200, batch_stress_name, batch_y in train_loader:\n",
        "        outputs = model(batch_species, batch_upstream200, batch_stress_name)\n",
        "        y_pred_train.extend(outputs.numpy())\n",
        "        y_true_train.extend(batch_y.numpy())\n",
        "\n",
        "    y_pred_test = []\n",
        "    y_true_test = []\n",
        "    for batch_species, batch_upstream200, batch_stress_name, batch_y in test_loader:\n",
        "        outputs = model(batch_species, batch_upstream200, batch_stress_name)\n",
        "        y_pred_test.extend(outputs.numpy())\n",
        "        y_true_test.extend(batch_y.numpy())\n",
        "\n",
        "    train_mse = mean_squared_error(y_true_train, y_pred_train)\n",
        "    test_mse = mean_squared_error(y_true_test, y_pred_test)\n",
        "\n",
        "    train_r2 = r2_score(y_true_train, y_pred_train)\n",
        "    test_r2 = r2_score(y_true_test, y_pred_test)\n",
        "\n",
        "print(f'Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}')\n",
        "print(f'Test MSE: {test_mse:.4f}, Test R²: {test_r2:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUQoS-adRJ_z",
        "outputId": "8b588244-f12c-414d-9dbf-f422cef2f510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MSE: 0.6530, Train R²: 0.1606\n",
            "Test MSE: 0.7088, Test R²: 0.0879\n"
          ]
        }
      ]
    }
  ]
}